---
title: "Classification"
subtitle: 'Classification of coalescent models'
output:
  prettydoc::html_pretty:
    toc: true
    theme: 'cayman'
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Aim of this code

Determine what are the best summaries using the SFS (and maybe the singleton distribution) to classify data in order to associate them with the right model.

For instance only the $n$-coalescent and the $\beta$-coalescent will be tested, and we will use knn, lda and qda to obtain classification model.

Also in the $n$-coalescent and beta the parameters n and alpha will take a range of value to determine if the selected summaries gives good results for any parameters for those 2 models.

The following would be to extend this to other models like population growth model, or selection, ...

# Setup
download of the package and other scripts
```{r, message = FALSE}
source('../simulations/simcoal.R')
require(phasty)
require(class)
require(MASS)
require(ggplot2)
```

# The ```nvbclass``` type function

```nvbclass()``` will simulate and return given summaries of the SFS and singleton distribution, in order to classify the two model later.

Is uses the ```summaries``` class function, there are different type of summaries function all given with a number to define wich summaries to use. 

##Inputs

```alpha```, the value of the parameter alpha for the beta coalescent

```FUN```, the type of summary function used.

```n``` the sample size

```replicate``` the number of time to simulate the model in order to have a 'theoretical' results.

## Outputs

The summary statistics considered, the order of the summaries are, first summary for $n$, first summary for $b$, then second summary for $n$ then for $b$, etc ...

```{r, cache = TRUE, cache.lazy = FALSE}
nvbclass <-  function(alpha, FUN = summaries1, n = 100, replicate = 1000){
    
    # initialisation of the matrix to stock he data
    nSFS <- matrix(0, ncol = n-1, nrow = replicate)
    bSFS <- matrix(0, ncol = n-1, nrow = replicate)
    
    nSD <- matrix(0, ncol = n, nrow = replicate)
    bSD <- matrix(0,ncol = n, nrow = replicate)
    
    # Simulation of the number of the SFS and singletond distribution for n
    for (i in 1:replicate){
        simul <- ncsim(n = n)
        nSFS[i,] <- simul[[3]]
        nSD[i,] <- singletondistr(simul)
    }
    
    # Same for Beta
    for (i in 1:replicate){
        simul <- bcsim(n = n, alpha = alpha)
        bSFS[i,] <- simul[[3]]
        bSD[i,] <- singletondistr(simul)
    }
    
    # calculation of the summaries 
    ## [ THIS PART CHANGES FROM ONE nbvclass FUNCTION TO THE OTHER ] ##
    results <- FUN(nSFS, bSFS, nSD, bSD)
    
    return(results)
}
```


```{r, cache = TRUE, cache.lazy = FALSE}
summaries1 = function(nSFS, bSFS, nSD, bSD){
    xn <- nSFS[, 1] / (apply(nSFS, 1, sum) + 0.00001)
    yn <- apply(nSFS,1,sum)
    
    xb <- bSFS[,1] / (apply(bSFS, 1, sum) + 0.00001)
    yb <- apply(bSFS, 1, sum)
    return(c(xn, xb, yn, yb))
}
```

# The classification function

This function will be used to creates the qda and knn models given a train and test dataset, for a specific nvbclass function.

## Inputs 

```alpha```, the value of the parameter alpha for the beta coalescent

```FUN```, the type of summary function used.

```n``` the sample size

```replicate``` the number of time to simulate the model in order to have a 'theoretical' results.

```{r, cache = TRUE, cache.lazy = FALSE}

classification_all <- function(alpha, n, FUN, replicate = 1000){
    
    # Simulate two data sets, one for the training part and one for the testing part with a ratio 80-20.
    size_train <- round(replicate * 0.8)
    size_test <- replicate - size_train
    train <- nvbclass(alpha = alpha, FUN = FUN, n = n, replicate = size_train)
    test <- nvbclass(alpha = alpha, FUN = FUN, n = n, replicate = size_test)
    nb_col <- length(test)/(size_test * 2)
    train <- matrix(train, ncol = nb_col)
    test <- matrix(test, ncol = nb_col)
    class <- factor(c(rep('n', size_train), rep('b', size_train)))
    
    data_knn <- knn(train = train, test = test, cl = class, k = 1)
    acc_knn <- (length(which(data_knn[1:size_test] == 'n')) + length(which(data_knn[size_test:(2*size_test)] == 'b'))) * 100 / (2 * size_test)
    
    
    plknn <- cbind(data.frame(test), data_knn)
    
    data_qda <- qda(train, class)
    pred_qda <- predict(data_qda, test)
    
    plqda <- cbind(data.frame(test), pred_qda$class)
    
    acc_qda <- (length(which(pred_qda$class[1:size_test] == 'n')) + length(which(pred_qda$class[size_test:(2*size_test)] == 'b'))) * 100 / (2*size_test)
    
    data_lda <- lda(train, class)
    pred_lda <- predict(data_lda, test)
    
    pllda <- cbind(data.frame(test), pred_lda$class)
    acc_lda <- (length(which(pred_lda$class[1:size_test] == 'n')) + length(which(pred_lda$class[size_test:(2*size_test)] == 'b'))) * 100 / (2*size_test)
    
    return(list(acc_knn, acc_qda, acc_lda, plknn, plqda, pllda))
}

```


This chunck, using the above function (```classification_all()```), will creates a dataframe with the accuracy for each set of parameter tested for the qda and knn models
```{r, cache = TRUE, cache.lazy = FALSE, fig.align= 'center'}
step <- 10
vecalpha <- seq(1, 1.9, length.out = step)
vecn <- seq(10, 50, length.out = step)
results <- data.frame(accuracy = rep(0, times = step*step*3), cl = c(rep('knn', step*step), rep('qda',step*step), rep('lda', step*step)), alpha = rep(vecalpha,3*step), n = rep(rep(vecn, each = step), 3))

for (n in 1:step){
    for (i in 1:step){
        giveclass <- classification_all(alpha = vecalpha[i], n = round(vecn[n]),
                                        FUN = summaries1, replicate = 1000)
        
        results$accuracy[i + (n-1)*step] <- giveclass[[1]]
        results$accuracy[i + (step*step) + (n-1)*step] <- giveclass[[2]]
        results$accuracy[i + (step*step)*2 + (n-1)*step] <- giveclass[[3]]
    }
}

cat('The mean of the accurcay for the KNN is: ', mean(results$accuracy[which(results$cl=='knn')]))
cat('The mean of the accurcay for the LDA is: ', mean(results$accuracy[which(results$cl=='lda')]))
cat('The mean of the accurcay for the QDA is: ', mean(results$accuracy[which(results$cl=='qda')]))

ggplot(data = results,aes(x = alpha, y = accuracy)) + geom_point(aes(col = cl)) + theme_bw() + xlab('Alpha') + ylab('Accuracy (%)')
```

We can see from this plot that the qda is always better than the knn.

Also as n increase the accuracy greatly increase, and that shows the importance of a big sample size to classify the data in the right model, especially if we suppose a close to 2 (but different) alpha.

Because the knn didnt work so well we can remove it from the classification function, and qda and lda give similar results so, we will keep lda from later on

```{r, cache = TRUE, cache.lazy = FALSE}
classification <- function(alpha, n, FUN, replicate = 1000){
    
    # Simulate two data sets, one for the training part and one for the testing part with a ratio 80-20.
    size_train <- round(replicate * 0.8)
    size_test <- replicate - size_train
    train <- nvbclass(alpha = alpha, FUN = FUN, n = n, replicate = size_train)
    test <- nvbclass(alpha = alpha, FUN = FUN, n = n, replicate = size_test)
    nb_col <- length(test) / (size_test * 2)
    train <- matrix(train, ncol = nb_col)
    test <- matrix(test, ncol = nb_col)
    class <- factor(c(rep('n', size_train), rep('b', size_train)))
    
    data_lda <- lda(train, class)
    pred_lda <- predict(data_lda, test)
    
    pllda <- cbind(data.frame(test), pred_lda$class)
    acc_lda <- (length(which(pred_lda$class[1:size_test] == 'n')) + length(which(pred_lda$class[size_test:(2*size_test)] == 'b'))) * 100 / (2*size_test)
    
    return(list(acc_lda, pllda))
}
```

```{r, cache = TRUE, cache.lazy = FALSE, echo = FALSE, fig.align= 'center'}

step <- 10
vecalpha <- seq(1, 1.9, length.out = step)
vecn <- seq(10, 100, length.out = step)
results <- data.frame(accuracy = rep(0, times = step*step),
                      cl = rep('qda', step*step),
                      alpha = rep(vecalpha,step),
                      n = rep(vecn, each = step))

for (n in 1:step){
    for (i in 1:step){
        giveclass <- classification(alpha = vecalpha[i], FUN = summaries1,
                                    n = round(vecn[n]), replicate = 1000)
        results$accuracy[i+(n-1)*step] <- giveclass[[1]]
    }
}

ggplot(data = results,aes(x = alpha, y = accuracy, col = n, group = n)) + geom_point() + geom_line() + theme_bw() + xlab('Alpha') + ylab('Accuracy (%)')

cat('The mean of the accurcay for this model is: ', mean(results$accuracy))
```

Know the goal will be to try to reach better summary to classify each model (for now only between the beta and the n coal)

# Tests of other summary to increase the accuracy 

Those summaries were quite logical because they take advantage of a big part of the information in the SFS regrouping.

But there is still some informations to extract from the SFS, but also the singleton distribution.

First we will try to add the proportion of the biggest singleton ind.

```{r, cache = TRUE, cache.lazy = FALSE}
summaries2 = function(nSFS, bSFS, nSD, bSD){
    
    xn <- nSFS[,1]/(apply(nSFS,1,sum)+0.00001)
    yn <- apply(nSFS,1,sum)
    zn <- nSD[,1]/(apply(nSD,1,sum)+0.00001)
    
    xb <- bSFS[,1]/(apply(bSFS,1,sum)+0.00001)
    yb <- apply(bSFS,1,sum)
    zb <- bSD[,1]/(apply(bSD,1,sum)+0.00001)
    
    return(c(xn,xb,yn,yb,zn,zb))
}
```

```{r, cache = TRUE, cache.lazy = FALSE, echo = FALSE, fig.align = 'center'}
n <- 50
step <- 10
vecalpha <- seq(1,1.9,length.out = step)
vecn <- seq(10,100,length.out = step)
results <- data.frame(accuracy = rep(0, times = step*step),
                     cl = rep('lda', step*step),
                     alpha = rep(vecalpha, step),
                     n = rep(vecn, each = step))

for (n in 1:step){
    for (i in 1:step){
        giveclass <- classification(alpha = vecalpha[i], FUN = summaries2,
                                 n = round(vecn[n]), replicate = 1000)
        results$accuracy[i+(n-1)*step] <- giveclass[[1]]
    }
}

ggplot(data = results,aes(x = alpha, y = accuracy, col = n, group = n)) + geom_point() + geom_line() + theme_bw() + xlab('Alpha') + ylab('Accuracy (%)')

cat('The mean of the accurcay for this model is: ', mean(results$accuracy))

```
The accuracy increased but in really small proportion (should we do a model selection ??)

Also one of the big problem is the value S. Indeed this value is higly dependant of the effective population size but the calculation of this value can change from one model to the other, for this reason it could be better either to have a correction or to use scaled value (but S cannot just be scaled by S). So maybe we can see if keeping the biggest singleton proportion and the proportion of singleton can be relevant.

```{r, cache = TRUE, cache.lazy = FALSE}
summaries3 <- function(nSFS, bSFS, nSD, bSD){
    
    xn <- nSFS[,1]/(apply(nSFS,1,sum)+0.00001)
    xb <- bSFS[,1]/(apply(bSFS,1,sum)+0.00001)
    
    return(c(xn,xb))
}
```

```{r, cache = TRUE, cache.lazy = FALSE, echo = FALSE, fig.align= 'center'}
n <- 50
step <- 10
vecalpha <- seq(1,1.9,length.out = step)
vecn <- seq(10,100,length.out = step)
results <- data.frame(accuracy = rep(0, times = step*step),
                     cl = rep('qda',step*step),
                     alpha = rep(vecalpha,step),
                     n = rep(vecn,each = step))

for (n in 1:step){
    for (i in 1:step){
        giveclass <- classification(alpha = vecalpha[i], FUN = summaries3,
                                 n = round(vecn[n]), replicate = 1000)
        results$accuracy[i+(n-1)*step] <- giveclass[[1]]
    }
}

ggplot(data = results,aes(x = alpha, y = accuracy, col = n, group = n)) + geom_point() + geom_line() + theme_bw() + xlab('Alpha') + ylab('Accuracy (%)')

cat('The mean of the accurcay for this model is: ', mean(results$accuracy))

```


Adding the tail of the SFS and the highest singleton.

```{r, cache = TRUE, cache.lazy = FALSE}
summaries4 <- function(nSFS, bSFS, nSD, bSD){
    
    n <- ncol(nSFS)-1
    xn <- nSFS[,1]/(apply(nSFS,1,sum)+0.00001)
    xb <- bSFS[,1]/(apply(bSFS,1,sum)+0.00001)
    yn <- apply(nSFS[,round(n-n/10):n],1,sum)/(apply(nSFS,1,sum)+0.00001)
    yb <- apply(bSFS[,round(n-n/10):n],1,sum)/(apply(bSFS,1,sum)+0.00001)
    zn <- nSD[,1]/(apply(nSD,1,sum)+0.00001)
    zb <- bSD[,1]/(apply(bSD,1,sum)+0.00001)
    
    return(c(xn,xb,zn,zb))
}
```

```{r, cache = TRUE, cache.lazy = FALSE, echo = FALSE, fig.align= 'center'}

step <- 10
vecalpha <- seq(1,1.9,length.out = step)
vecn <- seq(10,100,length.out = step)
results <- data.frame(accuracy = rep(0, times = step*step),
                     cl = rep('qda',step*step),
                     alpha = rep(vecalpha,step),
                     n = rep(vecn,each = step))

for (n in 1:step){
    for (i in 1:step){
        giveclass <- classification(alpha = vecalpha[i], FUN = summaries4,
                                 n = round(vecn[n]), replicate = 1000)
        results$accuracy[i+(n-1)*step] <- giveclass[[1]]
        print(n)
        print(i)
    }
}

ggplot(data = results,aes(x = alpha, y = accuracy, col = n, group = n)) + geom_point() + geom_line() + theme_bw() + xlab('Alpha') + ylab('Accuracy (%)')

cat('The mean of the accurcay for this model is: ', mean(results$accuracy))

```
