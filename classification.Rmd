---
title: "Classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Aim of this code

Determine what are the bet summaries of the SFS (maybe the singleton distribution) to classify data in order to associate them with the right model.

For instance only the n and the beta will be tested, and we will use knn and qda to obtain classification model.

Also in the n and beta the parameters n and alpha will take a range of value to determine if the selected summaries gives good results for any parameters for those 2 models.

The following would be to extend this to other models like population growth model, or selection, ...

# Setup
download of the package and other scripts
```{r}
source('simcoal.R')
require(class)
require(MASS)
require(ggplot2)
```


## The classification function

This function will be used to creates the qda and knn models given a train and test dataset.

```{r}
classification = function(alpha, n, replicate = 1000){
  train = nvbclass(alpha = alpha, n=n, replicate = replicate)
  test = nvbclass(alpha = alpha, n=n, replicate = replicate)

train = matrix(train, ncol=2)
test = matrix(test, ncol=2)
class = factor(c(rep('n',replicate),rep('b',replicate)))

a = knn(train = train, test = test, cl = class, k = 1)
predknn = (length(which(a[1:replicate]=='n'))+length(which(a[replicate:(2*replicate)]=='b')))*100/(2*replicate)


plknn = cbind(data.frame(test), a)
names(plknn) = c('a','b','c')

q = qda(train, class)
z = predict(q, test)


plqda = cbind(data.frame(test), z$class)
names(plqda) = c('a','b','c')


predqda = (length(which(z$class[1:replicate]=='n'))+length(which(z$class[replicate:(2*replicate)]=='b')))*100/(2*replicate)

return(list(predknn,predqda,plknn,plqda))
}
```


This chunck, using the above function (classification), will creates a dataframe with the accuracy for each set of parameter tested for the qda and knn models
```{r}
n=50
step = 10
vecalpha = seq(1,1.9,length.out = step)
vecn = seq(10,200,length.out = step)
results = data.frame(accuracy = rep(0, times = step*step*2),
                     cl = c(rep('knn',step*step),rep('qda',step*step)),
                     alpha = rep(vecalpha,2*step),
                     n = rep(rep(vecn,each = step),2))

for (n in 1:step){
  for (i in 1:step){
  plout=classification(alpha = vecalpha[i], n = round(vecn[n]), replicate = 100)
  #print(ggplot(data=plout[[3]], aes(x = a, y = b)) + geom_point(aes(colour=c)))
  #print(ggplot(data=plout[[4]], aes(x = a, y = b)) + geom_point(aes(colour=c)))
  results$accuracy[i+(n-1)*step] = plout[[1]]
  results$accuracy[i+(step*step)+(n-1)*step] = plout[[2]]
  print(n*10+i)
}
}


ggplot(data = results,aes(x = alpha, y = accuracy)) + geom_point(aes(shape=cl, col = as.character(n)))
```

We can see from this plot that the qda is always better than the knn.

Also as n increase the accuracy greatly increase, and that shows the importance of a big sample size to classify the data in the right model, especially if we suppose a close to 2 (but different) alpha.

# Tests of other summary to increase the accuracy 

Those summaries were quite logical because they take advantage of a big part of the information in the SFS regrouping.

But there is still some informations to extract from the SFS, but also the singleton distribution.

